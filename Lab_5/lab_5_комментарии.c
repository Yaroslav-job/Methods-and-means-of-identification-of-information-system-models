#define _POSIX_C_SOURCE 199309L   // Разрешаем использование POSIX-функций (например, clock_gettime) с нужным стандартом

#include <stdio.h>                // Подключаем стандартную библиотеку ввода-вывода (printf и т.д.)
#include <stdlib.h>               // Подключаем стандартную библиотеку (atoi, malloc, free и т.д.)
#include <time.h>                 // Подключаем библиотеку для работы со временем (clock_gettime)
#include <mpi.h>                  // Подключаем заголовок библиотеки MPI

int main(int argc, char *argv[]) {   // Точка входа в программу; MPI требует (argc, argv) для своей инициализации
    int N;                            // Размер матриц (N x N)
    double *A = NULL;                 // Указатель на матрицу A (будет динамически выделена)
    double *B = NULL;                 // Указатель на матрицу B
    double *C = NULL;                 // Указатель на матрицу C — результат умножения A*B
    int i, j, k;                      // Счётчики циклов
    int rank;                         // Номер текущего процесса внутри MPI_COMM_WORLD
    int size;                         // Общее количество процессов в MPI_COMM_WORLD

    if (argc < 2) {                   // Проверяем, передал ли пользователь аргумент командной строки (N)
        printf("Использование: %s N\n", argv[0]); // Если нет — выводим подсказку по использованию программы
        return 1;                     // Выходим с кодом ошибки
    }

    N = atoi(argv[1]);                // Преобразуем строку argv[1] в целое число и сохраняем в N
    if (N <= 0) {                     // Проверяем, что размер матрицы положительный
        printf("N должен быть > 0\n");// Если нет — сообщаем об ошибке
        return 1;                     // И завершаем программу
    }

    MPI_Init(&argc, &argv);           // Инициализация MPI; после этого можно вызывать остальные MPI-функции

    MPI_Comm_rank(MPI_COMM_WORLD, &rank); // Определяем номер текущего процесса (rank) в общем коммуникаторе
    MPI_Comm_size(MPI_COMM_WORLD, &size); // Определяем общее число процессов (size) в коммуникаторе

    A = (double*)malloc((size_t)N * N * sizeof(double)); // Выделяем память под матрицу A размером N*N элементов типа double
    B = (double*)malloc((size_t)N * N * sizeof(double)); // Выделяем память под матрицу B
    C = (double*)malloc((size_t)N * N * sizeof(double)); // Выделяем память под матрицу C
    if (!A || !B || !C) {            // Проверяем, успешно ли выделилась память (хотя бы один указатель не NULL)
        printf("Не хватает памяти (rank %d)\n", rank); // Сообщаем об ошибке, указывая номер процесса
        MPI_Abort(MPI_COMM_WORLD, 1); // Аварийно завершаем все MPI-процессы (гарантированная остановка всей программы)
        return 1;                    // Возвращаем код ошибки (на всякий случай, хотя после MPI_Abort сюда обычно не дойдём)
    }

    if (rank == 0) {                  // Инициализация матриц A и B производится только на процессе с рангом 0 (корневой)
        for (i = 0; i < N; i++) {     // Внешний цикл по строкам матрицы
            for (j = 0; j < N; j++) { // Внутренний цикл по столбцам матрицы
                A[i*N + j] = i + 1;   // Заполняем A: каждая строка содержит одно и то же число (i+1)
                B[i*N + j] = 1.0 / (j + 1); // Заполняем B: значение зависит только от номера столбца (1/(j+1))
                C[i*N + j] = 0.0;     // Инициализируем C нулями (для всех элементов)
            }
        }
    } else {                          // Для всех процессов, кроме нулевого
        for (i = 0; i < N*N; i++) {   // Проходим по всем элементам C
            C[i] = 0.0;               // Обнуляем матрицу C (на самом деле не обязательно, но это аккуратно)
        }
        // Матрицы A и B здесь пока не инициализированы, их значения будут получены через MPI_Bcast
    }

    MPI_Bcast(A, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD); // Рассылаем матрицу A от процесса 0 ко всем процессам
                                                      // - первый параметр: указатель на данные
                                                      // - второй: количество элементов
                                                      // - третий: тип данных (double)
                                                      // - четвёртый: ранг процесса-отправителя (0)
                                                      // - пятый: коммуникатор (все процессы)

    MPI_Bcast(B, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD); // Аналогично рассылаем матрицу B всем процессам

    { // Начинаем блок, где описана логика разбиения работы и умножения

        int rows_per_process;          // Базовое количество строк, которое достанется каждому процессу
        int remainder;                 // Остаток строк (если N не делится на size без остатка)
        int extra;                     // Дополнительная строка (0 или 1), которую некоторым процессам придётся взять
        int start_row;                 // Номер первой строки матрицы A, которую обрабатывает данный процесс
        int end_row;                   // Номер строки "после последней" (диапазон [start_row, end_row))

        struct timespec ts_start;      // Структура для хранения времени начала вычислений
        struct timespec ts_end;        // Структура для хранения времени конца вычислений

        rows_per_process = N / size;   // Делим общее число строк на количество процессов (целочисленное деление)
        remainder = N % size;          // Считаем остаток — сколько строк не уместилось в ровные блоки

        extra = (rank < remainder) ? 1 : 0; // Первые 'remainder' процессов получают по одной дополнительной строке
                                           // Если rank < remainder, extra = 1, иначе 0

        // Формула для вычисления начальной строки:
        // rank * rows_per_process — базовый сдвиг
        // (rank < remainder ? rank : remainder) — учитываем, сколько дополнительных строк получили процессы с меньшим rank
        start_row = rank * rows_per_process + (rank < remainder ? rank : remainder);

        // Конечная строка — это start_row плюс базовое количество строк и возможная дополнительная
        end_row = start_row + rows_per_process + extra;

        MPI_Barrier(MPI_COMM_WORLD);   // Синхронизация всех процессов: ждём, пока все дойдут до этой точки
                                       // Нужна, чтобы замер времени начинался примерно одновременно

        clock_gettime(CLOCK_MONOTONIC, &ts_start); // Запоминаем время начала вычислений (монотонные часы, не зависящие от системного времени)

        // Основной тройной цикл умножения матриц – но только по строкам [start_row, end_row)
        for (i = start_row; i < end_row; i++) { // Цикл по строкам матрицы A / C, которые принадлежат текущему процессу
            for (j = 0; j < N; j++) {           // Цикл по столбцам матрицы B / C
                double s = 0.0;                 // Переменная для накопления суммы произведений по k
                for (k = 0; k < N; k++) {       // Цикл по индексу k (перебор элементов строки A и столбца B)
                    s += A[i*N + k] * B[k*N + j]; // Добавляем к сумме произведение A[i][k] * B[k][j]
                }
                C[i*N + j] = s;                 // Записываем результат в элемент C[i][j]
            }
        }

        if (rank != 0) {                        // Если это не корневой процесс
            int rows_local;                     // Количество строк, посчитанных локально данным процессом
            rows_local = end_row - start_row;   // Это разница между конечной и начальной строкой

            // Отправляем посчитанную часть матрицы C процессу 0:
            // адрес начала блока данных: &C[start_row * N]
            // количество элементов: rows_local * N
            MPI_Send(&C[start_row * N], rows_local * N, MPI_DOUBLE,
                     0, 0, MPI_COMM_WORLD);     // Отправляем данные (tag = 0) процессу с рангом 0
        } else {                                // Если это корневой процесс (rank == 0)
            int r;                              // Счётчик для перебора остальных процессов

            for (r = 1; r < size; r++) {        // Перебираем все процессы, кроме 0-го
                int rp;                         // Количество строк, которые должен был считать процесс r
                int sr;                         // Стартовая строка для процесса r

                // rp — такое же вычисление, как и для текущего процесса выше:
                // базовое число строк + 1, если r < remainder
                rp = rows_per_process + (r < remainder ? 1 : 0);

                // sr — начальная строка процесса r (по той же формуле, что для start_row)
                sr = r * rows_per_process + (r < remainder ? r : remainder);

                // Получаем от процесса r блок его строк матрицы C
                MPI_Recv(&C[sr * N], rp * N, MPI_DOUBLE,
                         r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
                         // Принимаем rp*N элементов типа double по тегу 0 от процесса r
                         // MPI_STATUS_IGNORE — мы не используем подробную информацию о статусе
            }
        }

        MPI_Barrier(MPI_COMM_WORLD);   // Снова синхронизируем процессы: ждём, пока все закончат вычисления и передачу данных

        clock_gettime(CLOCK_MONOTONIC, &ts_end); // Фиксируем время окончания вычислений

        if (rank == 0) {               // Только корневой процесс выводит результаты
            double sec;                // Переменная для хранения времени в секундах

            // Вычисляем разницу во времени в секундах:
            // (разность секунд) + (разность наносекунд / 1e9)
            sec = (ts_end.tv_sec - ts_start.tv_sec)
                + (ts_end.tv_nsec - ts_start.tv_nsec) / 1e9;

            printf("n = %d\n", N);     // Выводим размер матрицы

            printf("C[0][0] = %.6f\n", C[0]);               // Выводим элемент C(0,0) для проверки корректности
            printf("C[0][%d] = %.6f\n", N-1, C[N-1]);       // Выводим элемент C(0, N-1)
            printf("C[%d][0] = %.6f\n", N-1, C[(N-1)*N+0]); // Выводим элемент C(N-1, 0)
            printf("C[%d][%d] = %.6f\n", N-1, N-1,
                   C[(N-1)*N + (N-1)]);                     // Выводим элемент C(N-1, N-1)

            printf("Время: %.6f сек\n", sec);               // Выводим измеренное время работы (только умножение и сборка)

            // Оценка производительности:
            // При умножении двух матриц N x N требуется примерно 2 * N^3 операций с плавающей точкой (умножения+сложения)
            // Делим количество операций на время и на 1e9, чтобы получить GFlops
            printf("Производительность: %.2f GFlops\n",
                   (2.0 * N * N * N) / (sec * 1e9));
        } // конец if (rank == 0)
    } // конец блока с вычислением и сбором результатов

    free(A);                          // Освобождаем память, выделенную под матрицу A
    free(B);                          // Освобождаем память под матрицу B
    free(C);                          // Освобождаем память под матрицу C

    MPI_Finalize();                   // Корректно завершаем работу MPI-библиотеки (после этого MPI-функции вызывать нельзя)
    return 0;                         // Возвращаем 0 — успешное завершение программы
}
